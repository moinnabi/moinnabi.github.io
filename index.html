<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- original template from from url=(0035)http://www.cs.berkeley.edu/~barron/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
	<meta name="viewport" content="width=800">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
     <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
	
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link rel="icon" type="image/png" href="http://www.cs.berkeley.edu/~barron/seal_icon.png">
	
    <title>Moin Nabi</title>
    
    <link href="/img/css" rel="stylesheet" type="text/css">
  </head>
  <body>
    <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
      <tbody><tr>
        <td>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="68%" valign="middle">
                <p align="center">
                   <name>Moin Nabi</name>
                  
                </p><p align="">I am a Principal Scientist at SAP AI CTO office, co-leading the <a href="https://www.sap.com/trends/machine-learning.html">SAP AI Research</a> team in Berlin.
</br>
		  </br>

		      Before that, I was a researcher at the University of Trento with <a href="http://disi.unitn.it/~sebe/">Nicu Sebe</a> and at the <a href="https://www.cs.washington.edu/">University of Washington</a> working with <a href="http://homes.cs.washington.edu/~ali/">Ali Farhadi</a>.
I did my PhD at the <a href="http://www.iit.it/">Italian Institute of Technology</a> where I was advised by Professor <a href="http://profs.sci.univr.it/~swan/">Vittorio Murino</a> and <a href="http://www0.cs.ucl.ac.uk/staff/m.pontil/">Massimiliano Pontil</a> from UCL.
	    </br></br>
		  I am so delighted to start computer vision research with <a href="https://explorecourses.stanford.edu/instructor/mehrdads">Mehrdad Shahshahani</a> at <a href="http://www.ipm.ac.ir/">IPM Vision Group</a>.
I did my masters in AI and my bachelors in software engineering in Iran.
	  </br>

<!--I had the opportunity to work under
I started Computer Vision with Mehrdad Shahshahani
Prior to my Ph.D., I spent one year as a research assistant at MI&V lab in <a href="http://www.test.com">Sharif University of Technology</a>. I was also fortunate enough to be advised by Professor <a href="http://www.test.com">Mehrdad Shahshahani</a> at the <a href="http://www.test.com">IPM Vision Group</a> from 2009 until 2011. I used to collaborate with IPPR lab at <a href="http://www.test.com">Amirkabir University of Technology</a> under supervision of Professor <a href="http://www.test.com">Mohammad Rahmati</a> as well.
I received my Master Degree on Artificial Intelligence from Tehran Polytechnic and my Bachelor Degree on Software Engineering from Shomal University at Amol (my home town). -->
                </p><p align="center">
<a href="mailto:m.nabi@sap.com">Email</a> &nbsp;/&nbsp;
<a href="./files/cv.pdf">CV</a> &nbsp;/&nbsp;
<!--<a href="https://scholar.google.it/citations?user=31seHAMAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp; -->
<a href="./files/Thesis_compressed.pdf">Thesis</a> &nbsp;/&nbsp;
<a href="https://scholar.google.com/citations?user=31seHAMAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp; 
<a href="https://it.linkedin.com/pub/moin-nabi/3b/492/aa5"> LinkedIn </a>
                </p>
              </td>
              <!--<td width="33%"><img src="./img/moin_pic_cool.jpg"></td>-->
				<td> <img src="./img/moin_pic_cool_2.jpg" style="width: 170;"></td></tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Research</heading>
                <!--<p> I work primarily on computer vision, but I am also interested in machine learning and pattern recognition. The central goal of my research is to use vast amounts of data to understand the underlying semantics and structure of visual contents. I am especially interested in learning and recognizing visual object categories and understanding human behaviors. I spent my Ph.D. working on learning mid-level representations for visual recognition (image and video understanding) and now, I am more focused on learning deep neural networks from noisy and incomplete multi-modal data.</p>-->
              <p>My research lies at the intersection of machine learning, computer vision, and natural language processing with an emphasis on learning Deep Neural Networks with minimal supervision and noisy/incomplete multi-modal data.
		      </br></br>
		      <span class="highlight"><strong>Internship Position: </strong> I like working with students. If you're a PhD student interested in a research internship working with me in Berlin, please send me an email with your CV and research interests.</span>
		      </p>
		    </td>
            </tr>
          </tbody></table>
<!--SECTION -->

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>News</heading>
<!-- 		      
<p> <strong>[2020.04.05]</strong> A <a href="https://arxiv.org/pdf/2005.00669.pdf">paper</a> on Contrastive Self-Supervised Learning for Commonsense Reasoning is accepted at <a href="http://www.acl2020.org">ACL 2020</a>. </p>		      
Topic Editor for the Identifying, Analyzing, and Overcoming Challenges in Vision and Language Research in the Frontiers Research Topics
Proud to be one of the co-Editors of a Special Issue on Language and Vision by Frontiers!
I serve as a topic editor for a Special Issue on Language and Vision by Frontiers!
	      

<!-- 	<p> <strong>[2021]</strong> Recently accepted papers - 2 @EMNLP, 1 @ICCV, 1 @NAACL, 1 @WACV, 1 @IPMI	 -->	   
		      
<p> <strong>[2021.09.06]</strong> I gave a <strong><a href="https://www.youtube.com/watch?v=pSuFwH5lw3E">talk</a></strong> on Continual Learning at the <a href="https://www.meetup.com/berlin-machine-learning/events/277956570/">Berlin ML MeetUp</a></a>.</p>
		      
<p> <strong>[2021.08.01]</strong> The <strong><a href="https://arxiv.org/abs/2108.01775">solo-learn</a></strong>, a library for self-supervised visual representation learning is <a href="https://github.com/vturrisi/solo-learn">online</a>.</p>		      		      
		      
<p> <strong>[2020.12.02]</strong> I serve as a topic editor for the <a href="https://www.frontiersin.org/research-topics/18532/identifying-analyzing-and-overcoming-challenges-in-vision-and-language-research">Special Issue on Language and Vision</a> by Frontiers.</p>		      
	      
<p> <strong>[2019.09.02]</strong> I am looking for a highly motivated PhD student to work on <a href="https://iid.unitn.it/education/admission/reserved-topic-scholarships">continual learning</a>.</p>		      
<!-- 
		      <!-- 			      
<p> <strong>[2019.07.22]</strong> A <a href="https://arxiv.org/pdf/1905.06242.pdf">paper</a> on Multi-Domain Learning is accepted at <a href="http://iccv2019.thecvf.com/">ICCV 2019</a>. </p>
		      
<p> <strong>[2019.05.14]</strong> A <a href="https://arxiv.org/pdf/1905.13497.pdf">paper</a> on commonsense reasoning is accepted at <a href="http://www.acl2019.org">ACL 2019</a>. </p>

<p> <strong>[2019.02.25]</strong> A <a href="https://arxiv.org/pdf/1904.03137.pdf">paper</a> on Continual Learning is accepted at <a href="http://cvpr2019.thecvf.com/">CVPR 2019</a>, Congratulations to Oleksiy! </p>		      
 -->
<p> <strong>[2019.12.01]</strong> Second workshop on <a href="https://sites.google.com/view/sivl2019/">Shortcomings in Vision and Language</a> is accepted at NAACL 2019.</p>
<!--  		      
<p> <strong>[2018.11.06]</strong> Three papers are accepted in <a href="http://wacv19.wacv.net/">WACV 2019</a>.</p>
 -->		      
<p> <strong>[2018.09.14]</strong> Our <a href="https://medium.com/sap-machine-learning-research/try-9e1ed9ae09ed">team</a> is among top-performing teams at the ECCV <a href="https://vizwiz.org/workshops/2018-workshop/">VizWiz Grand Challenge</a>.</p>
		      
<p> <strong>[2018.04.01]</strong> A workshop on <a href="https://sites.google.com/view/sivl/">Shortcomings in Vision and Language</a> is accepted at ECCV 2018.</p>
<!--  
<p> <strong>[2018.02.01]</strong> A paper on <a href="https://arxiv.org/abs/1605.07651.pdf">Self-paced Deep Learning</a> is accepted to PAMI 2018.</p>
-->
<p> <strong>[2017.09.19]</strong> Our <a href="https://arxiv.org/abs/1708.09644">paper</a> is awarded the <strong><a href="http://2017.ieeeicip.org/AwardFinalist.asp">Microsoft Best Student Paper</a></strong> in ICIP 2017.</p>	
<!-- 
<p> <strong>[2017.04.01]</strong> Two papers are accepted in <a href="http://acl2017.org/">ACL 2017</a>. </p>           
<p> <strong>[2016.11.11]</strong> The <a href="https://github.com/hosseinm/med">Motion Emotion Dataset (MED)</a> is online!</p>
 -->   
              </td>
            </tr>
          </tbody></table>
<!--SECTION -->

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Selected Publications</heading>
		      <p>For the full list of my publications please visit my <a href="https://scholar.google.com/citations?hl=en&user=31seHAMAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> page.</p>

              </td>
            </tr>
          </tbody></table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="7">
		<tbody>
		   <tr>
			   
			   
<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/cvpr2023.png" alt="PontTuset" width="140" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href=".">
	<papertitle>Semi-supervised Learning Made Simple with Self-supervised Clustering</papertitle></a><br>
			E. Fini, P. Astolfi, K. Alahari, X. Alameda-Pineda, J. Mairal, <strong>M. Nabi</strong>, E. Ricci<br>
                  <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023 &nbsp; <br>
		<a href="http://lear.inrialpes.fr/~alahari/papers/fini23.pdf">PDF</a> / 
		<a href=".">project page</a> /
		  <a href="https://github.com/DonkeyShot21">code</a>

                </p><p></p>
<!--
			      <p>In this paper, we proposed a novel crowd dataset with both annotations of abnormal crowd behavior and crowd emotion. We also presented a method which exploits jointly the complimentary information of these two task, outperforming all baselines of both tasks significantly.
-->
		</p><p></p>
                <p></p>
              </td>
            </tr>

			   
			   
<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/jmlr_22.png" alt="PontTuset" width="150" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="https://jmlr.org/papers/v23/21-1155.html">
	<papertitle>Solo-learn: A Library of Self-supervised Methods for Visual Representation Learning</papertitle></a><br>
			V. Costa, E. Fini, <strong>M. Nabi</strong>, N. Sebe, E. Ricci<br>
                  <em>Journal of Machine Learning Research (JMLR)</em>, 2022 &nbsp; <br> <!-- <font color="red"><strong>(Oral)</strong></font><br> -->
		<font color="red"><strong> >1.2k stars on GitHub</strong></font><br>
				
		<a href="https://arxiv.org/abs/2108.01775">PDF</a> / 
		  <a href="https://github.com/vturrisi/solo-learn">code</a> /
		  <a href="https://solo-learn.readthedocs.io/en/latest/">Documentation</a>
                </p><p></p>
<!--
			      <p>In this paper, we proposed a novel crowd dataset with both annotations of abnormal crowd behavior and crowd emotion. We also presented a method which exploits jointly the complimentary information of these two task, outperforming all baselines of both tasks significantly.
-->
		</p><p></p>
                <p></p>
              </td>
            </tr>			   
			   
<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/tpami_22.png" alt="PontTuset" width="150" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="https://ieeexplore.ieee.org/abstract/document/9745778">
	<papertitle>Uncertainty-aware Contrastive Distillation for Incremental Semantic Segmentation</papertitle></a><br>
			G. Yang, E. Fini, D. Xu, P. Rota; M. Ding, <strong>M. Nabi</strong>, X. Alameda-Pineda, E. Ricci<br>
                  <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2022 &nbsp; <br> <!-- <font color="red"><strong>(Oral)</strong></font><br> -->
		<a href="https://arxiv.org/abs/2203.14098">PDF</a> / 
		  <a href="https://github.com/ygjwd12345/UCD">code</a> 
                </p><p></p>
<!--
			      <p>In this paper, we proposed a novel crowd dataset with both annotations of abnormal crowd behavior and crowd emotion. We also presented a method which exploits jointly the complimentary information of these two task, outperforming all baselines of both tasks significantly.
-->
		</p><p></p>
                <p></p>
              </td>
            </tr>	


<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="https://raw.githubusercontent.com/SAP-samples/acl2022-self-contrastive-decorrelation/main/images/scd_illustration.png" alt="PontTuset" width="150" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="https://arxiv.org/abs/2203.07847">
	<papertitle>SCD: Self-Contrastive Decorrelation for Sentence Embeddings</papertitle></a><br>
			T. Klein, <strong>M. Nabi</strong><br>
                  <em>Association for Computational Linguistics (ACL)</em>, 2022 &nbsp; <br>
                  <a href="https://arxiv.org/pdf/2203.07847.pdf">PDF</a> /
		  <a href="https://github.com/SAP-samples/acl2022-self-contrastive-decorrelation">code</a>

                </p><p></p>
<!--
			      <p>In this paper, we proposed a novel crowd dataset with both annotations of abnormal crowd behavior and crowd emotion. We also presented a method which exploits jointly the complimentary information of these two task, outperforming all baselines of both tasks significantly.
-->
		</p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/iccv_21_3.png" alt="PontTuset" width="150" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="https://ncd-uno.github.io/">
	<papertitle>A Unified Objective for Novel Class Discovery</papertitle></a><br>
			E. Fini, E. Sangineto, S. Lathuilire, Z. Zhong, <strong>M. Nabi</strong>, E. Ricci<br>
                  <em>IEEE International Conference on Computer Vision (ICCV)</em>, 2021 &nbsp; <font color="red"><strong>(Oral)</strong></font><br>
		<a href=".">PDF</a> / 
		<a href="https://ncd-uno.github.io/">project page</a> /
		  <a href="https://github.com/DonkeyShot21/UNO">code</a>

                </p><p></p>
<!--
			      <p>In this paper, we proposed a novel crowd dataset with both annotations of abnormal crowd behavior and crowd emotion. We also presented a method which exploits jointly the complimentary information of these two task, outperforming all baselines of both tasks significantly.
-->
		</p><p></p>
                <p></p>
              </td>
            </tr>


<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="https://raw.githubusercontent.com/SAP-samples/emnlp2021-attention-contrastive-learning/main/img/attention_contrastive.png" alt="PontTuset" width="140" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="https://arxiv.org/abs/2109.05108">
	<papertitle>Attention-based Contrastive Learning for Winograd Schemas</papertitle></a><br>
			T. Klein, <strong>M. Nabi</strong><br>
                  <em>Findings of Empirical Methods in Natural Language Processing (EMNLP)</em>, 2021 &nbsp; <br>
                  <a href="https://arxiv.org/pdf/2109.05108.pdf">PDF</a> /
		  <a href="https://github.com/SAP-samples/emnlp2021-attention-contrastive-learning/">code</a>

                </p><p></p>
<!--
			      <p>In this paper, we proposed a novel crowd dataset with both annotations of abnormal crowd behavior and crowd emotion. We also presented a method which exploits jointly the complimentary information of these two task, outperforming all baselines of both tasks significantly.
-->
		</p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="https://raw.githubusercontent.com/SAP-samples/emnlp2021-contrastive-refinement/main/img/refinement_task.png" alt="PontTuset" width="150" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="https://arxiv.org/abs/2109.05105">
	<papertitle>Towards Zero-shot Commonsense Reasoning with Self-supervised Refinement of Language Models</papertitle></a><br>
			T. Klein, <strong>M. Nabi</strong><br>
                  <em>Empirical Methods in Natural Language Processing (EMNLP)</em>, 2021 &nbsp; <br>
                  <a href="https://arxiv.org/pdf/2109.05105.pdf">PDF</a> /
		  <a href="hhttps://github.com/SAP-samples/emnlp2021-contrastive-refinement/">code</a>

                </p><p></p>
<!--
			      <p>In this paper, we proposed a novel crowd dataset with both annotations of abnormal crowd behavior and crowd emotion. We also presented a method which exploits jointly the complimentary information of these two task, outperforming all baselines of both tasks significantly.
-->
		</p><p></p>
                <p></p>
              </td>
            </tr>
			   
			   
<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/naacl21_2.png" alt="PontTuset" width="140" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="https://www.aclweb.org/anthology/2021.naacl-main.192/">
	<papertitle>EaSe: A Diagnostic Tool for VQA based on Answer Diversity</papertitle></a><br>
			S. Jolly, S. Pezzelle, <strong>M. Nabi</strong><br>
                  <em>North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</em>, 2021 &nbsp; <br>
                  <a href="https://www.aclweb.org/anthology/2021.naacl-main.192.pdf">PDF</a> /
		  <a href="https://github.com/shailzajolly/EaSe">code</a>

                </p><p></p>
<!--
			      <p>In this paper, we proposed a novel crowd dataset with both annotations of abnormal crowd behavior and crowd emotion. We also presented a method which exploits jointly the complimentary information of these two task, outperforming all baselines of both tasks significantly.
-->
		</p><p></p>
                <p></p>
              </td>
            </tr>	
			   
<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/eccv20_2.png" alt="PontTuset" width="150" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="https://arxiv.org/abs/2008.01510">
	<papertitle>Online Continual Learning under Extreme Memory Constraints</papertitle></a><br>
			E. Fini, S. Lathuilire, E. Sangineto, <strong>M. Nabi</strong>, E. Ricci<br>
                  <em>European Conference on Computer Vision (ECCV)</em>, 2020 &nbsp; <br>
                  <a href="https://arxiv.org/pdf/2008.01510.pdf">PDF</a> /
		  <a href="https://github.com/DonkeyShot21/batch-level-distillation">code</a>

                </p><p></p>
<!--
			      <p>In this paper, we proposed a novel crowd dataset with both annotations of abnormal crowd behavior and crowd emotion. We also presented a method which exploits jointly the complimentary information of these two task, outperforming all baselines of both tasks significantly.
-->
		</p><p></p>
                <p></p>
              </td>
            </tr>			   


<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/acl20_2.png" alt="PontTuset" width="115" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="https://arxiv.org/abs/2005.00669">
	<papertitle>Contrastive Self-Supervised Learning for Commonsense Reasoning</papertitle></a><br>
			T. Klein, <strong>M. Nabi</strong><br>
                  <em>Association for Computational Linguistics (ACL)</em>, 2020 &nbsp; <br>
                  <a href="https://arxiv.org/pdf/2005.00669.pdf">PDF</a> /
		  <a href="https://github.com/SAP-samples/acl2020-commonsense">code</a>

                </p><p></p>
<!--
			      <p>In this paper, we proposed a novel crowd dataset with both annotations of abnormal crowd behavior and crowd emotion. We also presented a method which exploits jointly the complimentary information of these two task, outperforming all baselines of both tasks significantly.
-->
		</p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/dgm_3.png" alt="PontTuset" width="150" style="border-style: none">
	              </td><td width="80%" valign="top">
	                <p><a href="https://arxiv.org/abs/1904.03137">
	<papertitle>Learning to Remember: A Synaptic Plasticity Driven Framework for Continual Learning</papertitle></a><br>O. Ostapenko, M. Puscas, T. Klein, P. Jahnichen, <strong>M. Nabi</strong><br>
                  <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2019 &nbsp; <br>
                  <a href="https://arxiv.org/pdf/1904.03137.pdf">PDF</a> /
		  <a href="https://github.com/SAP-samples/machine-learning-dgm">code</a> /
		<a href="https://medium.com/sap-machine-learning-research/learning-to-remember-a-synaptic-plasticity-driven-framework-for-continual-learning-41b0d54d86af">blog</a>				
                </p><p></p>
<!--
			   <p>In this paper we propose a self-paced learning protocol for weakly-supervised object detection. The main idea is to iteratively select a subset of samples that are most likely correct, which are used for training. We show results on Pascal VOC and ImageNet, outperforming the previous state of the art on both datasets.
-->
		</p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
	              <td width="20%"><img src="./img/iccv.png" alt="PontTuset" width="140" style="border-style: none">
	              </td><td width="80%" valign="top">
	                <p><a href="https://arxiv.org/abs/1905.06242">
	<papertitle>Budget-Aware Adapters for Multi-Domain Learning</papertitle></a><br>R. Berriel, S. Lathuili, <strong>M. Nabi</strong>, T. Klein, T. Oliveira, N. Sebe, E. Ricci<br>
                  <em>IEEE International Conference on Computer Vision (ICCV)</em>, 2019 &nbsp; <br>
                  <a href="https://arxiv.org/pdf/1905.06242.pdf">PDF</a> /
		  <a href="https://github.com/rodrigoberriel/budget-aware-adapters">code</a>
                </p><p></p>
<!--
			   <p>In this paper we propose a self-paced learning protocol for weakly-supervised object detection. The main idea is to iteratively select a subset of samples that are most likely correct, which are used for training. We show results on Pascal VOC and ImageNet, outperforming the previous state of the art on both datasets.
-->
		</p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/acl19.png" alt="PontTuset" width="140" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="https://arxiv.org/abs/1905.13497">
	<papertitle>Attention Is (not) All You Need for Commonsense Reasoning</papertitle></a><br>
			T. Klein, <strong>M. Nabi</strong><br>
                  <em>Association for Computational Linguistics (ACL)</em>, 2019 &nbsp; <br>
                  <a href="https://arxiv.org/pdf/1905.13497.pdf">PDF</a> /
		  <a href="https://github.com/SAP-samples/acl2019-commonsense">code</a>

                </p><p></p>
<!--
			      <p>In this paper, we proposed a novel crowd dataset with both annotations of abnormal crowd behavior and crowd emotion. We also presented a method which exploits jointly the complimentary information of these two task, outperforming all baselines of both tasks significantly.
-->
		</p><p></p>
                <p></p>
              </td>
            </tr>
			   

<!--Next paper should be listed in bottom -->
	              <td width="20%"><img src="./img/pami_new.png" alt="PontTuset" width="140" style="border-style: none">
	              </td><td width="80%" valign="top">
	                <p><a href="https://arxiv.org/abs/1605.07651">
	<papertitle>Self-Paced Deep Learning for Weakly Supervised Object Detection</papertitle></a><br>E. Sangineto*, <strong>M. Nabi</strong>*, D. Culibrk and N. Sebe<br>
                  <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2018 &nbsp; <br>
                  <a href="https://arxiv.org/pdf/1605.07651.pdf">PDF</a> /
		  <a href="https://github.com/moinnabi/SelfPacedDeepLearning">code</a>
                </p><p></p>
<!--
			   <p>In this paper we propose a self-paced learning protocol for weakly-supervised object detection. The main idea is to iteratively select a subset of samples that are most likely correct, which are used for training. We show results on Pascal VOC and ImageNet, outperforming the previous state of the art on both datasets.
-->
		</p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
	              <td width="20%"><img src="./img/dpfl_3.png" alt="PontTuset" width="140" style="border-style: none">
	              </td><td width="80%" valign="top">
	                <p><a href="https://arxiv.org/abs/1712.07557">
	<papertitle>Differentially Private Federated Learning: A Client Level Perspective</papertitle></a><br>R. Gayer, T. Klein, <strong>M. Nabi</strong><br>
                  <em>Conference on Neural Information Processing Systems (NIPS)</em>, 2017 &nbsp; <br>
                  <em>Workshop on Machine Learning on the Phone and Consumer Devices</em> &nbsp; <br>
<!--		<font color="red"><strong> >900 Citations</strong></font><br>      -->
				
				
                  <a href="https://arxiv.org/pdf/1712.07557.pdf">PDF</a> /
		  <a href="https://paperswithcode.com/paper/differentially-private-federated-learning-a">code</a> /
				<a href="https://medium.com/sap-machine-learning-research/client-sided-differential-privacy-preserving-federated-learning-1fab5242d31b">blog</a> /
				<a href="https://miro.medium.com/max/2000/1*aq_Mx_XflA-bSZvAdfUEfA.jpeg">poster</a>
                </p><p></p>
<!--
			   <p>In this paper we propose a self-paced learning protocol for weakly-supervised object detection. The main idea is to iteratively select a subset of samples that are most likely correct, which are used for training. We show results on Pascal VOC and ImageNet, outperforming the previous state of the art on both datasets.
-->
		</p><p></p>
                <p></p>
              </td>
            </tr>
			   

<!--Next paper should be listed in bottom 
	              <td width="20%"><img src="./img/CNNPlugPlay.jpg" alt="PontTuset" width="140" style="border-style: none">
	              </td><td width="80%" valign="top">
	                <p><a href="https://arxiv.org/abs/1610.00307">
	<papertitle>Plug-and-Play CNN for Crowd  Analysis: An Application in Abnormal Event Detection</papertitle></a><br>
			M. Ravanbakhsh, <strong>M. Nabi</strong>, Mousavi, E. Sangineto and N. Sebe<br>

		<em>IEEE Winter Conference on Applications of Computer Vision (WACV)</em>, 2018 &nbsp; <br>
                  <a href="https://arxiv.org/pdf/1610.00307v1.pdf">PDF</a> /
                  <a href="#">bibtex</a>
                </p><p></p>

			      <p>We present a novel measure-based method which allows measuring the local abnormality in a video by combining semantic information (inherited from existing CNN models) with low-level Optical Flow. The proposed method is validated on challenging abnormality detection datasets and the showed the superiority of our method compared to the state-of-the-arts.

		</p><p></p>
                <p></p>
              </td>
            </tr>
-->

<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/foil.png" alt="PontTuset" width="130" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="https://foilunitn.github.io/">
	<papertitle>FOIL it! Find One mismatch between Image and Language caption</papertitle></a><br>
			R. Shekhar, S. Pezzelle, A. Herbelot, <strong>M. Nabi</strong>, E. Sangineto, R. Bernardi<br>
                  <em>Association for Computational Linguistics (ACL)</em>, 2017 &nbsp; <font color="red"><strong>(Oral)</strong></font><br>
		<a href="http://aclweb.org/anthology/P17-1024">PDF</a> /
		<a href="https://foilunitn.github.io/content/aclPresentation-RaviShekhar.pdf">talk</a> /
		  <a href="https://foilunitn.github.io/">code</a> /
		<a href="https://foilunitn.github.io/">data</a>	/
		<a href="https://medium.com/sap-machine-learning-research/deep-vision-language-integration-242742fa4552">blog</a>
                </p><p></p>
<!--
			      <p>In this paper, we proposed a novel crowd dataset with both annotations of abnormal crowd behavior and crowd emotion. We also presented a method which exploits jointly the complimentary information of these two task, outperforming all baselines of both tasks significantly.
-->
		</p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
	              <td width="22%"><img src="./img/selfCS.png" alt="PontTuset" width="125" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Self-Crowdsourcing Training for Relation Extraction</papertitle></a><br>
			A. Abad, <strong>M. Nabi</strong>, A. Moschitti<br>
                  <em>Association for Computational Linguistics (ACL)</em>, 2017 &nbsp; <br>
		<a href="http://aclweb.org/anthology/P17-2082">PDF</a> /
		  <a href="#">bibtex</a>
                </p><p></p>
<!--
			      <p>In this paper, we proposed a novel crowd dataset with both annotations of abnormal crowd behavior and crowd emotion. We also presented a method which exploits jointly the complimentary information of these two task, outperforming all baselines of both tasks significantly.
-->
		</p><p></p>
                <p></p>
              </td>
            </tr>


<!--Next paper should be listed in bottom -->
	              <td width="20%"><img src="./img/icip17.png" alt="PontTuset" width="140" style="border-style: none">
	              </td><td width="80%" valign="top">
	                <p><a href="https://ieeexplore.ieee.org/document/8296547/">
	<papertitle>Abnormal Event Detection in Videos using Generative Adversarial Nets</papertitle></a><br>
			M. Ravanbakhsh, <strong>M. Nabi</strong>, E. Sangineto L. Mercenaro, C. Regazzoni, N. Sebe<br>
                  <em>IEEE International Conference on Image Processing (ICIP)</em>, 2017 &nbsp; <br>
		<font color="red"><strong>Microsoft Best Student Paper Award</strong></font><br>

                  <a href="https://arxiv.org/pdf/1708.09644.pdf">PDF</a> /
		<a href="https://github.com/otoofim/ABNORMAL_EVENT_DETECTION-IN_VIDEOS_USING_GENERATIVE_ADVERSARIAL_NETS">code</a> /
                  <a href="#">bibtex</a>
                </p><p></p>
<!--
			      <p>We present a novel measure-based method which allows measuring the local abnormality in a video by combining semantic information (inherited from existing CNN models) with low-level Optical Flow. The proposed method is validated on challenging abnormality detection datasets and the showed the superiority of our method compared to the state-of-the-arts.
-->
		</p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/sigir.png" alt="PontTuset" width="130" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Autonomous Crowdsourcing through Human-Machine Collaborative Learning</papertitle></a><br>
			A. Abad, <strong>M. Nabi</strong>, A. Moschitti<br>
                  <em>Special Interest Group on Information Retrieval (SIGIR)</em>, 2017 &nbsp; <br>
		<a href="https://dl.acm.org/citation.cfm?id=3080666">PDF</a> /
		  <a href="#">bibtex</a>
                </p><p></p>
<!--
			      <p>In this paper, we proposed a novel crowd dataset with both annotations of abnormal crowd behavior and crowd emotion. We also presented a method which exploits jointly the complimentary information of these two task, outperforming all baselines of both tasks significantly.
-->
		</p><p></p>
                <p></p>
              </td>
            </tr>


<!--Next paper should be listed in bottom 
	              <td width="25%"><img src="./img/adapt.png" alt="PontTuset" width="140" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>A Cross-modal Adaptation Approach for Brain Decoding</papertitle></a><br>
			P. Ghaemmaghami, <strong>M. Nabi</strong>, Y. Yan, G. Riccardi, and N. Sebe<br>
                  <em>International Conference on Acoustics, Speech, Signal Processing (ICASSP)</em>, 2017 <br>
                  <a href="https://ieeexplore.ieee.org/abstract/document/7952300/">PDF</a> /
                  <a href="#">bibtex</a>
                </p><p></p>

			      <p>In this paper, an adaptation paradigm is employed in order to transfer knowledge from visual domain to brain domain. We experimentally show that such adaptation procedure leads to improved results for the object recognition task in the brain domain, outperforming significantly the results achieved by the brain features alone.

			      </p><p></p>
                <p></p>
              </td>
            </tr>

-->

<!--Next paper should be listed in bottom 
	              <td width="25%"><img src="./img/BrainDecode.jpg" alt="PontTuset" width="130" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Sparse-coded Cross-domain Adaptation from Visual to Brain Domain</papertitle></a><br>
			P. Ghaemmaghami, <strong>M. Nabi</strong>, Y. Yan and N. Sebe<br>
                  <em>IEEE International Conference on Pattern Recognition (ICPR)</em>, 2016 &nbsp; <font color="red"><strong>(Oral)</strong></font><br>
                  <a href="./files/ICPR2016_paper.pdf">PDF</a> /
                  <a href="#">slies</a> /
                  <a href="./files/ICPR2016.bib">bibtex</a>
                </p><p></p>

			      <p>In this paper, an adaptation paradigm is employed in order to transfer knowledge from visual domain to brain domain. We experimentally show that such adaptation procedure leads to improved results for the object recognition task in the brain domain, outperforming significantly the results achieved by the brain features alone.

			      </p><p></p>
                <p></p>
              </td>
            </tr>
-->

<!--Next paper should be listed in bottom 
	              <td width="25%"><img src="./img/Emotion.jpg" alt="PontTuset" width="120" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="https://arxiv.org/abs/1607.07646">
	<papertitle>Emotion-Based Crowd Representation for Abnormality Detection</papertitle></a><br>
			H.R. Rabiee, J. Haddadnia, H. Mousavi, <strong>M. Nabi</strong>, V. Murino and N. Sebe<br>
                  <em>arXiv:1607.07646</em>, 2016 &nbsp; <br>
		<a href="https://arxiv.org/pdf/1607.07646v1">PDF</a> /
		  <a href="https://github.com/hosseinm/med">dataset</a> /
                  <a href="http://dblp.uni-trier.de/rec/bib2/journals/corr/RabieeHMNMS16.bib">bibtex</a>
                </p><p></p>

			      <p>In this paper, we proposed a novel crowd dataset with both annotations of abnormal crowd behavior and crowd emotion. We also presented a method which exploits jointly the complimentary information of these two task, outperforming all baselines of both tasks significantly.

		</p><p></p>
                <p></p>
              </td>
            </tr>
-->

<!--Next paper should be listed in bottom 
	              <td width="25%"><img src="./img/FineGrained.jpg" alt="PontTuset" width="130" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="http://ieeexplore.ieee.org/document/7738074/">
	<papertitle>Novel Dataset for Fine-grained Abnormal Behavior Understanding in Crowd</papertitle></a><br>
			H.R. Rabiee, J. Haddadnia, H. Mousavi, M. Kalantarzadeh, <strong>M. Nabi</strong>, V. Murino<br>
                  <em> IEEE Advanced Video and Signal-based Surveillance (AVSS)</em>, 2016 &nbsp; <br>
                  <a href="./files/AVSS2016_paper.pdf">PDF</a> /
                  <a href="./files/AVSS2016_poster.pdf">poster</a> /
                  <a href="https://github.com/hosseinm/med">dataset</a> /
                  <a href="./files/AVSS2016.bib">bibtex</a>
                </p><p></p>
					
                <p>This work presents a novel crowd dataset annotated by fine-grained abnormal behavior categoriy labels. We also evaluated two state-of-the-art methods on our dataset, showing that our dataset can be effectively used as a benchmark for the fine-grained abnormality detection problem.
                
                </p><p></p>
                <p></p>
              </td>
            </tr>
-->

<!--Next paper should be listed in bottom 
	              <td width="25%"><img src="./img/CNNaware.jpg" alt="PontTuset" width="140" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="https://arxiv.org/abs/1609.09220">
	<papertitle>CNN-aware Binary Map For General Image Segmentation</papertitle></a><br>
	M. Ravanbakhsh, H. Mousavi, <strong>M. Nabi</strong>, M. Rastegari and C. Regazzoni<br>
                  <em>IEEE International Conference on Image Processing (ICIP)</em>, 2016 &nbsp; <br>
		<font color="red"><strong>Best Paper / Student Paper Award Finalist (7 / ~2000 submissions)</strong></font><br>
                  <a href="./files/ICIP2016_paper.pdf">PDF</a> /
                  <a href="./files/ICIP2016_poster.pdf">poster</a> /
                  <a href="./files/ICIP2016_slide.pdf">slides</a> /
                  <a href="https://github.com/matt-rb">code</a> /
                  <a href="./files/ICIP2016.bib">bibtex</a>
                </p><p></p>
		
                <p>In this paper we introduce a novel method for general semantic segmentation that can benefit from general semantics of Convolutional Neural Network (CNN). Experiments show that our segmentation algorithm outperform the state-of-the-art non-semantic segmentation methods by large margin.
                
		</p><p></p>
                <p></p>
              </td>
            </tr>
-->
<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/SubModels.PNG" alt="PontTuset" width="130" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="http://ieeexplore.ieee.org/document/7298988/?reload=true&arnumber=7298988">
	<papertitle>Learning with Dataset Bias in Latent Subcategory Models</papertitle></a><br>
                  D. Stamos, S. Martelli, <strong>M. Nabi</strong>, A. McDonald, V. Murino, M. Pontil<br>
                  <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2015 &nbsp; <br>
                  <a href="./files/CVPR2015_paper.pdf">PDF</a> /
                  <a href="./files/CVPR2015_abstract.pdf">abstract</a> /
                  <a href="./files/CVPR2015.bib">bibtex</a>
                </p><p></p>
		<!--
                <p>We present a multi-task learning framework which provides a means to borrow statistical strength from the datasets while reducing their inherent bias. In experiments we demonstrate that our method, when tested on PASCAL, LabelMe, Caltech101 and SUN in a leave-one-dataset-out fashion, achieves an average improvement of over 6.5% over state-of-the-arts.
		-->
                </p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom 
	              <td width="25%"><img src="./img/CM.jpg" alt="PontTuset" width="130" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="http://ieeexplore.ieee.org/document/7351223/">
	<papertitle>Crowd Motion Monitoring Using Tracklet-based Commotion Measure</papertitle></a><br>
<strong>M. Nabi</strong>*, H. Mousavi*, H. Kiani, A. Perina and V. Murino<br>
                  <em>IEEE International Conference on Image Processing (ICIP)</em>, 2015 &nbsp; <br>
                  <a href="./files/ICIP2015_paper.pdf">PDF</a> /
                  <a href="./files/ICIP2015_poster.pdf">poster</a> /
                  <a href="./files/ICIP2015_video.mp4">video</a> /
                  <a href="./files/ICIP2015.bib">bibtex</a>
                </p><p></p>
		
                <p>We present a tracklet-based measure to capture the commotion of a crowd motion for the task of abnormality detection in crowd.
		
<small>*Authors contributed equally</small>

		</p><p></p>
                <p></p>
              </td>
            </tr>
-->

<!--Next paper should be listed in bottom 
	              <td width="25%"><img src="./img/iHOT.jpg" alt="PontTuset" width="130" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="http://link.springer.com/chapter/10.1007%2F978-3-319-23234-8_66#page-1">
	<papertitle>Abnormality Detection with Improved Histogram of Oriented Tracklets</papertitle></a><br>
                  H. Mousavi, <strong>M. Nabi</strong>, H. Kiani, A. Perina and V. Murino<br>
                  <em>International Conference on Image Analysis and Processing (ICIAP)</em>, 2015 &nbsp; <br>
                  <a href="./files/ICIAP2015_paper.pdf">PDF</a> /
                  <a href="./files/ICIAP2015_poster.pdf">poster</a> /
                  <a href="./files/ICIAP2015.bib">bibtex</a>
                </p><p></p>
		
                <p>In this paper, we presented an efficient video descriptor for the task of abnormality detection in the crowded environments and achieved the-state-of-the-art results on the available datasets.
		                
		</p><p></p>
                <p></p>
              </td>
            </tr>
-->

<!--Next paper should be listed in bottom 
	              <td width="25%"><img src="./img/Tposelet.jpg" alt="PontTuset" width="140" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="http://dl.acm.org/citation.cfm?id=2586288">
	<papertitle>Temporal Poselets for Collective Activity Detection and Recognition</papertitle></a><br><strong>M. Nabi</strong>, A. Del Bue, V. Murino<br>			
                  <em>IEEE International Conference on Computer Vision Workshops</em>, 2013 &nbsp; <font color="red"><strong>(Oral)</strong></font><br>
                  <a href="./files/ICCVW2013_paper.pdf">PDF</a> /
                  <a href="./files/ICCVW2013_slides.pdf">slide</a> /
                  <a href="https://youtu.be/jkkVZqgm-UE">video</a> /
                  <a href="./files/ICCVW2013_code.tar.gz">code</a> /
                  <a href="./files/ICCVW2013.bib">bibtex</a>
                </p><p></p>

<p>We present a mid-level motion representation based on actication patterns of Poselets detectors over time (Temporal Poselet), for detecting/recognizing the activity of a group of people in crowd environments.

		</p><p></p>
                <p></p>
              </td>
            </tr>

-->
<!--Next paper should be listed in bottom 
	              <td width="25%"><img src="./img/BagOfPoselet.jpg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Human Action Recognition in Still Images using Bag of Latent Poselets</papertitle>*</a><br><strong>M. Nabi</strong>, M. Rahmati<br>
                  <em>9th European Conference on Visual Media Production (CVMP)</em>, 2012 &nbsp;<br>
                  <a href="./files/CVMP2012_abstract.pdf">PDF</a> /
                  <a href="./files/CVMP2012.bib">bibtex</a>
                </p><p></p>
                <p>We represent human body poses in a single images by extracting the Poselet activation vectors on it, and recognize human activities in still images using the proposed bag of latent Poselets.
</br></br>
<small>*This work is based on my MS thesis at AUT.</small>
		</p><p></p>
                <p></p>
              </td>
            </tr>
-->

<!--SECTION 

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Miscellaneous</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody><tr>
-->

<!--Next paper should be listed in bottom 
	              <td width="25%"><img src="./img/PhDthesis.jpg" alt="PontTuset" width="140" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="http://arxiv.org/abs/1512.07314">
	<papertitle>Mid-level Representation for Visual Recognition</papertitle></a><br>
                  <strong>Moin Nabi</strong><br>
                  <em>Ph.D. Dissertation </em>, 2015 &nbsp; <br>
                  <a href="./files/PhD_thesis.pdf">PDF</a> /
                  <a href="./files/thesis_slide.pdf">slides</a> /
                  <a href="https://www.youtube.com/watch?v=6IY-0swKaiM">talk</a> /
                </p><p></p>
		  
                <p>This thesis targets employing mid-level representations for different high-level visual
recognition tasks, both in image and video understanding.
		 
                </p><p></p>
                <p></p>
              </td>
            </tr>
-->

<!--Next paper should be listed in bottom 
	              <td width="25%"><img src="./img/WeblyPatch_2.jpg" alt="PontTuset" width="130" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Webly-supervised Subcategoty-aware Discriminative Patch</papertitle></a><br><strong>M. Nabi</strong>, S. Divvala, A. Farhadi<br>
                  <em>Technical Report</em>*, 2014 &nbsp; <br>
                  <a href="./files/UW2014_TR.pdf">PDF</a> /
                  <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a>
                </p><p></p>
			      
<p>We study discovering a set of discriminative patches in subcategories of an object category, then train them in a webly-supervised fashion.
</br>			      
<small>*This work was done while I was at UW.</small>

                </p><p></p>
                <p></p>
              </td>
            </tr>
-->

<!--Next paper should be listed in bottom 
	              <td width="25%"><img src="./img/stock.jpg" alt="PontTuset" width="160" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Stock trend prediction using Twin Gaussian Process regression</papertitle></a><br>M. Mojaddady, <strong>M. Nabi</strong>, S. Khadivi<br>
                  <em>Technical Report</em>, 2011 &nbsp;<br>
                  <a href="./files/stock.pdf">PDF</a> /
                  <a href="./files/stock.bib">bibtex</a>
                </p><p></p>
                <p>
		</p><p></p>
                <p></p>
              </td>
            </tr>
-->

<!--Next paper should be listed in bottom -->
<!--
	              <td width="25%"><img src="./img/hafezedges.gif" alt="PontTuset" width="140" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>A Fuzzy Approach to Image Processing</papertitle></a><br><strong>M. Nabi</strong><br>
                  <em>Technical Report</em>, 2008 &nbsp;<br>
                  <a href="./files/fuzzy.pdf">PDF</a>
                </p><p></p>
                <p>
		</p><p></p>
                <p></p>
              </td>
            </tr>
-->

<!--Next paper should be listed in bottom 
	              <td width="25%"><img src="./img/matlab3.jpg" alt="PontTuset" width="140" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>A Turorial on Digital Image Processing using MATLAB</papertitle></a><br><strong>M. Nabi</strong><br>
                  <em>National Digital Image Processing Workshop</em>, 2008 &nbsp;<br>
                  <a href="./files/tutorial.zip">PDF</a> /
		  <a href="./files/code.zip">code</a> /
		  <a href="./files/images.tar.gz">data</a>
                </p><p></p>
                <p>
		</p><p></p>
                <p></p>
              </td>
            </tr>

-->


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <br>
                <p align="right"><font size="3">
			Erd&ouml;s = 3 (via two paths)
                  </font>
                <br>
		<p align="right"><font size="2">
                  <a href="http://www.cs.berkeley.edu/~barron/">Thanks Jon!</a>
                  </font>
                </p>
              </td>
            </tr>
          </tbody></table>
          <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
                    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
                    
          </script><script src="./img/ga.js" type="text/javascript"></script> <script type="text/javascript">
            try {
                    var pageTracker = _gat._getTracker("UA-7580334-1");
                    pageTracker._trackPageview();
                    } catch(err) {}
                    
          </script>
        </td>
      </tr>
    </tbody></table>
  

</body></html>
